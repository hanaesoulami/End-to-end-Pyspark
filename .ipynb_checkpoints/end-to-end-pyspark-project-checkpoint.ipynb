{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file out to JSON format\n",
    "#departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:35:35.238143Z",
     "iopub.status.busy": "2022-10-04T18:35:35.237678Z",
     "iopub.status.idle": "2022-10-04T18:36:50.744320Z",
     "shell.execute_reply": "2022-10-04T18:36:50.743103Z",
     "shell.execute_reply.started": "2022-10-04T18:35:35.238053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ligne 1: {sys.executable} : commande introuvable\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-04T18:36:50.747161Z",
     "iopub.status.busy": "2022-10-04T18:36:50.746766Z",
     "iopub.status.idle": "2022-10-04T18:36:56.983636Z",
     "shell.execute_reply": "2022-10-04T18:36:56.982061Z",
     "shell.execute_reply.started": "2022-10-04T18:36:50.747121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/23 12:24:08 WARN Utils: Your hostname, hanae-Latitude-5490, resolves to a loopback address: 127.0.1.1; using 138.195.47.30 instead (on interface wlp2s0)\n",
      "25/05/23 12:24:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/23 12:24:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark as sp\n",
    "\n",
    "sc = sp.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:36:56.987186Z",
     "iopub.status.busy": "2022-10-04T18:36:56.986316Z",
     "iopub.status.idle": "2022-10-04T18:36:57.168493Z",
     "shell.execute_reply": "2022-10-04T18:36:57.166333Z",
     "shell.execute_reply.started": "2022-10-04T18:36:56.987127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x72261af87310>\n"
     ]
    }
   ],
   "source": [
    "#import SparkSeccion pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#print my_spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/hanae/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/hanae/.local/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hanae/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hanae/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hanae/.local/lib/python3.10/site-packages (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de DataFrame Pandas en Vue Temporaire Spark\n",
    "\n",
    "Dans cette cellule, nous effectuons les étapes suivantes :\n",
    "\n",
    "1. Création d'un DataFrame `pandas` contenant des valeurs aléatoires.\n",
    "2. Conversion de ce DataFrame en DataFrame `Spark`.\n",
    "3. Enregistrement du DataFrame Spark comme **vue temporaire** dans le catalogue Spark.\n",
    "\n",
    "Cela permet d'interroger les données via des requêtes SQL Spark.\n",
    "\n",
    "### État du catalogue Spark :\n",
    "\n",
    "- **Avant** : Aucune table enregistrée (`[]`).\n",
    "- **Après** : Une table temporaire `temp` est visible dans le catalogue Spark :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:36:57.172754Z",
     "iopub.status.busy": "2022-10-04T18:36:57.171664Z",
     "iopub.status.idle": "2022-10-04T18:37:03.348656Z",
     "shell.execute_reply": "2022-10-04T18:37:03.347401Z",
     "shell.execute_reply.started": "2022-10-04T18:36:57.172713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "[Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   faa                           name        lat        lon   alt  tz dst\n",
      "0  04G              Lansdowne Airport  41.130472 -80.619583  1044  -5   A\n",
      "1  06A  Moton Field Municipal Airport  32.460572 -85.680028   264  -5   A\n",
      "2  06C            Schaumburg Regional  41.989341 -88.101243   801  -6   A\n",
      "3  06N                Randall Airport  41.431912 -74.391561   523  -5   A\n",
      "4  09J          Jekyll Island Airport  31.074472 -81.427778    11  -4   A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin du fichier .csv\n",
    "csv_file_path = '/home/hanae/Téléchargements/airports.csv'\n",
    "\n",
    "# Lire le fichier Excel en utilisant openpyxl\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Afficher les 5 premières lignes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données dans un DataFrame Spark, et affichage des premières lignes du DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:03.350415Z",
     "iopub.status.busy": "2022-10-04T18:37:03.349952Z",
     "iopub.status.idle": "2022-10-04T18:37:04.908662Z",
     "shell.execute_reply": "2022-10-04T18:37:04.907365Z",
     "shell.execute_reply.started": "2022-10-04T18:37:03.350372Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/hanae/Téléchargements/airports.csv'\n",
    "\n",
    "#Read in the airports path\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "airports.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le type de l'objet airports est un DataFrame de Spark, il représente une collection de données structurées, similaire à une table dans une base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:04.910490Z",
     "iopub.status.busy": "2022-10-04T18:37:04.910011Z",
     "iopub.status.idle": "2022-10-04T18:37:04.927911Z",
     "shell.execute_reply": "2022-10-04T18:37:04.926230Z",
     "shell.execute_reply.started": "2022-10-04T18:37:04.910443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le type de l'objet airports\n",
    "type(airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste des bases de données disponibles dans le catalogue Spark, permettant d'explorer les bases de données actuellement accessibles dans la session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:04.931522Z",
     "iopub.status.busy": "2022-10-04T18:37:04.930593Z",
     "iopub.status.idle": "2022-10-04T18:37:05.248720Z",
     "shell.execute_reply": "2022-10-04T18:37:05.247373Z",
     "shell.execute_reply.started": "2022-10-04T18:37:04.931469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/home/hanae/pyspark-en-to-end/spark-warehouse')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste des tables disponibles dans la base de données active de la session Spark, permettant d'explorer les tables accessibles dans la session en cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:05.250587Z",
     "iopub.status.busy": "2022-10-04T18:37:05.250103Z",
     "iopub.status.idle": "2022-10-04T18:37:05.463671Z",
     "shell.execute_reply": "2022-10-04T18:37:05.462305Z",
     "shell.execute_reply.started": "2022-10-04T18:37:05.250538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier CSV dans un DataFrame Spark, en utilisant la première ligne comme en-tête (header=True). Affichage des premières lignes du DataFrame flights pour visualiser les données chargées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:05.469400Z",
     "iopub.status.busy": "2022-10-04T18:37:05.468866Z",
     "iopub.status.idle": "2022-10-04T18:37:06.065131Z",
     "shell.execute_reply": "2022-10-04T18:37:06.063688Z",
     "shell.execute_reply.started": "2022-10-04T18:37:05.469352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.csv('/home/hanae/Téléchargements/flights_small.csv', header=True)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:06.071712Z",
     "iopub.status.busy": "2022-10-04T18:37:06.069128Z",
     "iopub.status.idle": "2022-10-04T18:37:06.298102Z",
     "shell.execute_reply": "2022-10-04T18:37:06.296167Z",
     "shell.execute_reply.started": "2022-10-04T18:37:06.071649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.name = flights.createOrReplaceTempView('flights')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:06.303502Z",
     "iopub.status.busy": "2022-10-04T18:37:06.302520Z",
     "iopub.status.idle": "2022-10-04T18:37:06.541702Z",
     "shell.execute_reply": "2022-10-04T18:37:06.540533Z",
     "shell.execute_reply.started": "2022-10-04T18:37:06.303444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 20 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame flights\n",
    "flights_df = spark.table('flights')\n",
    "print(flights_df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter une nouvelle colonne appelée duration_hrs au DataFrame flights. \n",
    "Cette colonne est calculée en divisant la colonne existante air_time (qui représente le temps de vol en minutes) par 60, afin de convertir la durée en heures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:37:06.543436Z",
     "iopub.status.busy": "2022-10-04T18:37:06.542928Z",
     "iopub.status.idle": "2022-10-04T18:37:07.183439Z",
     "shell.execute_reply": "2022-10-04T18:37:07.182164Z",
     "shell.execute_reply.started": "2022-10-04T18:37:06.543388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|               2.2|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|              1.85|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|1.3833333333333333|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|2.1166666666666667|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|2.0166666666666666|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|               1.5|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|1.6333333333333333|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|2.1666666666666665|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|2.1166666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|              2.15|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|               1.5|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|1.2666666666666666|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|              1.85|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#include a new column called duration_hrs\n",
    "flights = flights.withColumn('duration_hrs', flights.air_time / 60)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T18:41:35.352509Z",
     "iopub.status.busy": "2022-10-04T18:41:35.349321Z",
     "iopub.status.idle": "2022-10-04T18:41:38.209621Z",
     "shell.execute_reply": "2022-10-04T18:41:38.208238Z",
     "shell.execute_reply.started": "2022-10-04T18:41:35.352420Z"
    }
   },
   "outputs": [],
   "source": [
    "#flights.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire et d'afficher les vols dont la distance dépasse 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:51.849945Z",
     "iopub.status.busy": "2022-09-23T19:34:51.849545Z",
     "iopub.status.idle": "2022-09-23T19:34:52.122581Z",
     "shell.execute_reply": "2022-09-23T19:34:52.121667Z",
     "shell.execute_reply.started": "2022-09-23T19:34:51.849913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    9| 26|     610|       -5|    1523|       65|     US| N127UW|   616|   SEA| PHL|     293|    2378|   6|    10| 4.883333333333334|\n",
      "|2014|   12|  4|     954|       -6|    1348|      -17|     HA| N395HA|    29|   SEA| OGG|     333|    2640|   9|    54|              5.55|\n",
      "|2014|    6|  4|    1115|        0|    1346|       -3|     AS| N461AS|   488|   SEA| SAN|     133|    1050|  11|    15| 2.216666666666667|\n",
      "|2014|    6| 26|    2054|       -1|    2318|       -6|     B6| N590JB|   907|   SEA| ANC|     179|    1448|  20|    54|2.9833333333333334|\n",
      "|2014|    6|  7|    1823|       -7|    2112|      -28|     AS| N512AS|   815|   SEA| LIH|     335|    2701|  18|    23| 5.583333333333333|\n",
      "|2014|    4| 30|     801|        1|    1757|       90|     AS| N407AS|    18|   SEA| MCO|     342|    2554|   8|     1|               5.7|\n",
      "|2014|   11| 29|     905|      155|    1655|      170|     DL| N824DN|  1598|   SEA| ATL|     229|    2182|   9|     5| 3.816666666666667|\n",
      "|2014|    6|  2|    2222|        7|      55|       15|     AS| N402AS|    99|   SEA| ANC|     190|    1448|  22|    22|3.1666666666666665|\n",
      "|2014|   11| 15|    1034|       -6|    1414|      -26|     AS| N589AS|   794|   SEA| ABQ|     139|    1180|  10|    34| 2.316666666666667|\n",
      "|2014|   10| 20|    1328|       -1|    1949|        4|     UA| N68805|  1212|   SEA| IAH|     228|    1874|  13|    28|               3.8|\n",
      "|2014|   12| 16|    1500|        0|    1906|       19|     US| N662AW|   500|   SEA| PHX|     151|    1107|  15|     0|2.5166666666666666|\n",
      "|2014|   11| 19|    1319|       -6|    1821|      -14|     DL| N309US|  2164|   PDX| MSP|     169|    1426|  13|    19| 2.816666666666667|\n",
      "|2014|    5| 21|     515|        0|     757|        0|     US| N172US|   593|   SEA| PHX|     143|    1107|   5|    15|2.3833333333333333|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter flights with a SQL string\n",
    "long_flights1 = flights.filter('distance > 1000')\n",
    "long_flights1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtre les vols longs (distance > 1000) en utilisant une condition booléenne appliquée directement à la colonne distance du DataFrame flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:52.864418Z",
     "iopub.status.busy": "2022-09-23T19:34:52.864048Z",
     "iopub.status.idle": "2022-09-23T19:34:53.124354Z",
     "shell.execute_reply": "2022-09-23T19:34:53.123501Z",
     "shell.execute_reply.started": "2022-09-23T19:34:52.864390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    9| 26|     610|       -5|    1523|       65|     US| N127UW|   616|   SEA| PHL|     293|    2378|   6|    10| 4.883333333333334|\n",
      "|2014|   12|  4|     954|       -6|    1348|      -17|     HA| N395HA|    29|   SEA| OGG|     333|    2640|   9|    54|              5.55|\n",
      "|2014|    6|  4|    1115|        0|    1346|       -3|     AS| N461AS|   488|   SEA| SAN|     133|    1050|  11|    15| 2.216666666666667|\n",
      "|2014|    6| 26|    2054|       -1|    2318|       -6|     B6| N590JB|   907|   SEA| ANC|     179|    1448|  20|    54|2.9833333333333334|\n",
      "|2014|    6|  7|    1823|       -7|    2112|      -28|     AS| N512AS|   815|   SEA| LIH|     335|    2701|  18|    23| 5.583333333333333|\n",
      "|2014|    4| 30|     801|        1|    1757|       90|     AS| N407AS|    18|   SEA| MCO|     342|    2554|   8|     1|               5.7|\n",
      "|2014|   11| 29|     905|      155|    1655|      170|     DL| N824DN|  1598|   SEA| ATL|     229|    2182|   9|     5| 3.816666666666667|\n",
      "|2014|    6|  2|    2222|        7|      55|       15|     AS| N402AS|    99|   SEA| ANC|     190|    1448|  22|    22|3.1666666666666665|\n",
      "|2014|   11| 15|    1034|       -6|    1414|      -26|     AS| N589AS|   794|   SEA| ABQ|     139|    1180|  10|    34| 2.316666666666667|\n",
      "|2014|   10| 20|    1328|       -1|    1949|        4|     UA| N68805|  1212|   SEA| IAH|     228|    1874|  13|    28|               3.8|\n",
      "|2014|   12| 16|    1500|        0|    1906|       19|     US| N662AW|   500|   SEA| PHX|     151|    1107|  15|     0|2.5166666666666666|\n",
      "|2014|   11| 19|    1319|       -6|    1821|      -14|     DL| N309US|  2164|   PDX| MSP|     169|    1426|  13|    19| 2.816666666666667|\n",
      "|2014|    5| 21|     515|        0|     757|        0|     US| N172US|   593|   SEA| PHX|     143|    1107|   5|    15|2.3833333333333333|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Filter flights with a boolean column\n",
    "long_flights2 = flights.filter(flights.distance > 1000 )\n",
    "long_flights2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les colonnes spécifiques des vols (numéro d'enregistrement, origine et destination) dans un nouveau DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:53.894824Z",
     "iopub.status.busy": "2022-09-23T19:34:53.894444Z",
     "iopub.status.idle": "2022-09-23T19:34:53.918375Z",
     "shell.execute_reply": "2022-09-23T19:34:53.917446Z",
     "shell.execute_reply.started": "2022-09-23T19:34:53.894777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|tailnum|origin|dest|\n",
      "+-------+------+----+\n",
      "| N846VA|   SEA| LAX|\n",
      "| N559AS|   SEA| HNL|\n",
      "| N847VA|   SEA| SFO|\n",
      "| N360SW|   PDX| SJC|\n",
      "| N612AS|   SEA| BUR|\n",
      "| N646SW|   PDX| DEN|\n",
      "| N422WN|   PDX| OAK|\n",
      "| N361VA|   SEA| SFO|\n",
      "| N309AS|   SEA| SAN|\n",
      "| N564AS|   SEA| ORD|\n",
      "| N323AS|   SEA| LAX|\n",
      "| N305AS|   SEA| PHX|\n",
      "| N433AS|   SEA| LAS|\n",
      "| N765AS|   SEA| ANC|\n",
      "| N713AS|   SEA| SFO|\n",
      "| N27205|   PDX| SFO|\n",
      "| N626AS|   SEA| SMF|\n",
      "| N8634A|   SEA| MDW|\n",
      "| N597AS|   SEA| BOS|\n",
      "| N215AG|   PDX| BUR|\n",
      "+-------+------+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Select the first set of columns as a string\n",
    "selected_1 = flights.select('tailnum', 'origin', 'dest')\n",
    "selected_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les colonnes d'origine, de destination et du transporteur dans un nouveau DataFrame temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:55.087361Z",
     "iopub.status.busy": "2022-09-23T19:34:55.086569Z",
     "iopub.status.idle": "2022-09-23T19:34:55.100721Z",
     "shell.execute_reply": "2022-09-23T19:34:55.099754Z",
     "shell.execute_reply.started": "2022-09-23T19:34:55.087327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| LAX|     VX|\n",
      "|   SEA| HNL|     AS|\n",
      "|   SEA| SFO|     VX|\n",
      "|   PDX| SJC|     WN|\n",
      "|   SEA| BUR|     AS|\n",
      "|   PDX| DEN|     WN|\n",
      "|   PDX| OAK|     WN|\n",
      "|   SEA| SFO|     VX|\n",
      "|   SEA| SAN|     AS|\n",
      "|   SEA| ORD|     AS|\n",
      "|   SEA| LAX|     AS|\n",
      "|   SEA| PHX|     AS|\n",
      "|   SEA| LAS|     AS|\n",
      "|   SEA| ANC|     AS|\n",
      "|   SEA| SFO|     AS|\n",
      "|   PDX| SFO|     UA|\n",
      "|   SEA| SMF|     AS|\n",
      "|   SEA| MDW|     WN|\n",
      "|   SEA| BOS|     AS|\n",
      "|   PDX| BUR|     OO|\n",
      "+------+----+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Select the second set of columns usinf df.col_name\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux filtres logiques pour ne garder que les vols qui partent de Seattle (SEA) et arrivent à Portland (PDX). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:55.870280Z",
     "iopub.status.busy": "2022-09-23T19:34:55.869913Z",
     "iopub.status.idle": "2022-09-23T19:34:55.877262Z",
     "shell.execute_reply": "2022-09-23T19:34:55.876224Z",
     "shell.execute_reply.started": "2022-09-23T19:34:55.870250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define first filter to only keep flights from SEA to PDX.\n",
    "FilterA = flights.origin == 'SEA'\n",
    "FilterB =flights.dest == 'PDX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applique deux filtres sur le DataFrame temp pour ne conserver que les vols partant de Seattle (SEA) et arrivant à Portland (PDX) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:56.576728Z",
     "iopub.status.busy": "2022-09-23T19:34:56.575183Z",
     "iopub.status.idle": "2022-09-23T19:34:56.786715Z",
     "shell.execute_reply": "2022-09-23T19:34:56.785482Z",
     "shell.execute_reply.started": "2022-09-23T19:34:56.576663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     AS|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     AS|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "+------+----+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected_2 = temp.filter(FilterA).filter(FilterB)\n",
    "selected_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crée un tableau qui calcule la vitesse moyenne de chaque vol (avg_speed), dans les deux sens, en divisant la distance par le temps de vol (converti en heures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:57.007783Z",
     "iopub.status.busy": "2022-09-23T19:34:57.006620Z",
     "iopub.status.idle": "2022-09-23T19:34:57.035406Z",
     "shell.execute_reply": "2022-09-23T19:34:57.034046Z",
     "shell.execute_reply.started": "2022-09-23T19:34:57.007744Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "|   PDX| DEN| N646SW|491.40495867768595|\n",
      "|   PDX| OAK| N422WN|             362.0|\n",
      "|   SEA| SFO| N361VA| 415.7142857142857|\n",
      "|   SEA| SAN| N309AS| 466.6666666666667|\n",
      "|   SEA| ORD| N564AS| 521.5151515151515|\n",
      "|   SEA| LAX| N323AS| 440.3076923076923|\n",
      "|   SEA| PHX| N305AS|431.29870129870125|\n",
      "|   SEA| LAS| N433AS| 409.6062992125984|\n",
      "|   SEA| ANC| N765AS|474.75409836065575|\n",
      "|   SEA| SFO| N713AS| 315.8139534883721|\n",
      "|   PDX| SFO| N27205| 366.6666666666667|\n",
      "|   SEA| SMF| N626AS|477.63157894736844|\n",
      "|   SEA| MDW| N8634A|481.38888888888886|\n",
      "|   SEA| BOS| N597AS| 516.4137931034483|\n",
      "|   PDX| BUR| N215AG| 441.6216216216216|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Create a table of the average speed of each flight both ways.\n",
    "#Calculate average speed by dividing the distance by the air_time (converted to hours).Use the .alias() method name\n",
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "speed_1 = flights.select('origin','dest','tailnum', avg_speed)\n",
    "speed_1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélectionner les colonnes (origin, dest, tailnum) et calculer la vitesse moyenne de chaque vol en divisant la distance par le temps de vol en heures, tout en renommant cette nouvelle colonne en avg_speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:57.581987Z",
     "iopub.status.busy": "2022-09-23T19:34:57.581579Z",
     "iopub.status.idle": "2022-09-23T19:34:57.760652Z",
     "shell.execute_reply": "2022-09-23T19:34:57.759859Z",
     "shell.execute_reply.started": "2022-09-23T19:34:57.581954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "|   PDX| DEN| N646SW|491.40495867768595|\n",
      "|   PDX| OAK| N422WN|             362.0|\n",
      "|   SEA| SFO| N361VA| 415.7142857142857|\n",
      "|   SEA| SAN| N309AS| 466.6666666666667|\n",
      "|   SEA| ORD| N564AS| 521.5151515151515|\n",
      "|   SEA| LAX| N323AS| 440.3076923076923|\n",
      "|   SEA| PHX| N305AS|431.29870129870125|\n",
      "|   SEA| LAS| N433AS| 409.6062992125984|\n",
      "|   SEA| ANC| N765AS|474.75409836065575|\n",
      "|   SEA| SFO| N713AS| 315.8139534883721|\n",
      "|   PDX| SFO| N27205| 366.6666666666667|\n",
      "|   SEA| SMF| N626AS|477.63157894736844|\n",
      "|   SEA| MDW| N8634A|481.38888888888886|\n",
      "|   SEA| BOS| N597AS| 516.4137931034483|\n",
      "|   PDX| BUR| N215AG| 441.6216216216216|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Using the Spark DataFrame method .selectExpr() \n",
    "speed_2 =flights.selectExpr('origin','dest','tailnum','distance/(air_time/60) as avg_speed')\n",
    "speed_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génèrer des statistiques descriptives sur les colonnes numériques du DataFrame flights, telles que la moyenne, l'écart-type, le minimum, le maximum et les quantiles (25%, 50%, 75%) de chaque colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:34:58.201603Z",
     "iopub.status.busy": "2022-09-23T19:34:58.201260Z",
     "iopub.status.idle": "2022-09-23T19:35:01.254691Z",
     "shell.execute_reply": "2022-09-23T19:35:01.253853Z",
     "shell.execute_reply.started": "2022-09-23T19:34:58.201576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, year: string, month: string, day: string, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: string, origin: string, dest: string, air_time: string, distance: string, hour: string, minute: string, duration_hrs: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir les colonnes (distance,air_time) en float et affiche ensuite les statistiques descriptives pour ces deux colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:01.256862Z",
     "iopub.status.busy": "2022-09-23T19:35:01.256182Z",
     "iopub.status.idle": "2022-09-23T19:35:01.661120Z",
     "shell.execute_reply": "2022-09-23T19:35:01.660009Z",
     "shell.execute_reply.started": "2022-09-23T19:35:01.256828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 14:23:04 ERROR Executor: Exception in task 0.0 in stage 46.0 (TID 47)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 5 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/23 14:23:04 WARN TaskSetManager: Lost task 0.0 in stage 46.0 (TID 47) (138.195.47.30 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 5 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/23 14:23:04 ERROR TaskSetManager: Task 0 in stage 46.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-05-23 14:23:04.816\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 5 in cell [57]\", \"line\": \"\", \"fragment\": \"__eq__\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o481.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"__eq__\\\" was called from\\nline 5 in cell [57]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/hanae/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/hanae/.local/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:DataFrameQueryContextLogger:[CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hanae/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 282, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/hanae/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 327, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o481.showString.\n",
      ": org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 5 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__eq__\" was called from\nline 5 in cell [57]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26862/583211016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Afficher les statistiques descriptives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'air_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__eq__\" was called from\nline 5 in cell [57]\n"
     ]
    }
   ],
   "source": [
    "# Remplacer les valeurs manquantes par une valeur par défaut (par exemple 0)\n",
    "flights = flights.na.fill({'distance': 0, 'air_time': 0})\n",
    "\n",
    "# Puis effectuer la conversion en float\n",
    "flights = flights.withColumn('distance', flights.distance.cast('float'))\n",
    "flights = flights.withColumn('air_time', flights.air_time.cast('float'))\n",
    "\n",
    "# Afficher les statistiques descriptives\n",
    "flights.describe('air_time', 'distance').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:01.662674Z",
     "iopub.status.busy": "2022-09-23T19:35:01.662302Z",
     "iopub.status.idle": "2022-09-23T19:35:01.996226Z",
     "shell.execute_reply": "2022-09-23T19:35:01.995182Z",
     "shell.execute_reply.started": "2022-09-23T19:35:01.662638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 14:15:06 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 43)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 4 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/23 14:15:06 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 43) (138.195.47.30 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 4 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/23 14:15:06 ERROR TaskSetManager: Task 0 in stage 42.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-05-23 14:15:06.049\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 4 in cell [57]\", \"line\": \"\", \"fragment\": \"__eq__\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o371.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"__eq__\\\" was called from\\nline 4 in cell [57]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/hanae/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/hanae/.local/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:DataFrameQueryContextLogger:[CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hanae/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 282, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/hanae/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 327, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o371.showString.\n",
      ": org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__eq__\" was called from\n",
      "line 4 in cell [57]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__eq__\" was called from\nline 4 in cell [57]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26862/191112893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Find the length of the shortest (in terms of distance) flight that left PDX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'PDX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__eq__\" was called from\nline 4 in cell [57]\n"
     ]
    }
   ],
   "source": [
    "#Find the length of the shortest (in terms of distance) flight that left PDX \n",
    "flights.filter(flights.origin =='PDX').groupBy().min('distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:01.998236Z",
     "iopub.status.busy": "2022-09-23T19:35:01.997942Z",
     "iopub.status.idle": "2022-09-23T19:35:02.207712Z",
     "shell.execute_reply": "2022-09-23T19:35:02.206633Z",
     "shell.execute_reply.started": "2022-09-23T19:35:01.998206Z"
    }
   },
   "outputs": [],
   "source": [
    "#Find the length of the longest (in terms of time) flight that left SEA\n",
    "flights.filter(flights.origin == 'SEA').groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:02.208994Z",
     "iopub.status.busy": "2022-09-23T19:35:02.208706Z",
     "iopub.status.idle": "2022-09-23T19:35:02.424965Z",
     "shell.execute_reply": "2022-09-23T19:35:02.424048Z",
     "shell.execute_reply.started": "2022-09-23T19:35:02.208963Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the average air time of Delta Airlines flights  that left SEA. \n",
    "flights.filter(flights.carrier == 'DL').filter(flights.origin == 'SEA').groupBy().avg('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:02.426328Z",
     "iopub.status.busy": "2022-09-23T19:35:02.426019Z",
     "iopub.status.idle": "2022-09-23T19:35:02.625584Z",
     "shell.execute_reply": "2022-09-23T19:35:02.624720Z",
     "shell.execute_reply.started": "2022-09-23T19:35:02.426296Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the total number of hours all planes in this dataset spent in the air by creating a column called duration_hrs\n",
    "flights.withColumn('duration_hrs', flights.air_time/60).groupBy().sum('duration_hrs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:02.627477Z",
     "iopub.status.busy": "2022-09-23T19:35:02.627165Z",
     "iopub.status.idle": "2022-09-23T19:35:02.638711Z",
     "shell.execute_reply": "2022-09-23T19:35:02.637624Z",
     "shell.execute_reply.started": "2022-09-23T19:35:02.627448Z"
    }
   },
   "outputs": [],
   "source": [
    "#Group by tailnum column\n",
    "by_plane = flights.groupBy('tailnum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:02.796527Z",
     "iopub.status.busy": "2022-09-23T19:35:02.795730Z",
     "iopub.status.idle": "2022-09-23T19:35:03.403972Z",
     "shell.execute_reply": "2022-09-23T19:35:03.403126Z",
     "shell.execute_reply.started": "2022-09-23T19:35:02.796478Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use the .count() method with no arguments to count the number of flights each plane made\n",
    "by_plane.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:03.713336Z",
     "iopub.status.busy": "2022-09-23T19:35:03.712958Z",
     "iopub.status.idle": "2022-09-23T19:35:03.726597Z",
     "shell.execute_reply": "2022-09-23T19:35:03.725288Z",
     "shell.execute_reply.started": "2022-09-23T19:35:03.713307Z"
    }
   },
   "outputs": [],
   "source": [
    "#group by origin column\n",
    "by_origin = flights.groupBy('origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:04.280375Z",
     "iopub.status.busy": "2022-09-23T19:35:04.279972Z",
     "iopub.status.idle": "2022-09-23T19:35:04.577760Z",
     "shell.execute_reply": "2022-09-23T19:35:04.576772Z",
     "shell.execute_reply.started": "2022-09-23T19:35:04.280341Z"
    }
   },
   "outputs": [],
   "source": [
    "#Find the .avg() of the air_time column to find average duration of flights from PDX and SEA\n",
    "by_origin.avg('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:04.757590Z",
     "iopub.status.busy": "2022-09-23T19:35:04.757216Z",
     "iopub.status.idle": "2022-09-23T19:35:04.780920Z",
     "shell.execute_reply": "2022-09-23T19:35:04.779569Z",
     "shell.execute_reply.started": "2022-09-23T19:35:04.757559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#convert to dep_delay to numeric column\n",
    "flights = flights.withColumn('dep_delay', flights.dep_delay.cast('float'))\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy('month', 'dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:05.848970Z",
     "iopub.status.busy": "2022-09-23T19:35:05.848591Z",
     "iopub.status.idle": "2022-09-23T19:35:06.184523Z",
     "shell.execute_reply": "2022-09-23T19:35:06.183435Z",
     "shell.execute_reply.started": "2022-09-23T19:35:05.848941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg('dep_delay').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:06.513171Z",
     "iopub.status.busy": "2022-09-23T19:35:06.512761Z",
     "iopub.status.idle": "2022-09-23T19:35:06.628304Z",
     "shell.execute_reply": "2022-09-23T19:35:06.626904Z",
     "shell.execute_reply.started": "2022-09-23T19:35:06.513137Z"
    }
   },
   "outputs": [],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:07.417613Z",
     "iopub.status.busy": "2022-09-23T19:35:07.417253Z",
     "iopub.status.idle": "2022-09-23T19:35:07.427933Z",
     "shell.execute_reply": "2022-09-23T19:35:07.426873Z",
     "shell.execute_reply.started": "2022-09-23T19:35:07.417584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed('faa','dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:08.474529Z",
     "iopub.status.busy": "2022-09-23T19:35:08.473864Z",
     "iopub.status.idle": "2022-09-23T19:35:08.876976Z",
     "shell.execute_reply": "2022-09-23T19:35:08.876058Z",
     "shell.execute_reply.started": "2022-09-23T19:35:08.474490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports= flights.join(airports, on='dest', how='leftouter')\n",
    "flights_with_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:15.001595Z",
     "iopub.status.busy": "2022-09-23T19:35:15.001235Z",
     "iopub.status.idle": "2022-09-23T19:35:15.278298Z",
     "shell.execute_reply": "2022-09-23T19:35:15.277373Z",
     "shell.execute_reply.started": "2022-09-23T19:35:15.001566Z"
    }
   },
   "outputs": [],
   "source": [
    "planes = spark.read.csv('../input/datasets-for-pyspark-project/planes.csv', header=True)\n",
    "planes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:15.973626Z",
     "iopub.status.busy": "2022-09-23T19:35:15.973259Z",
     "iopub.status.idle": "2022-09-23T19:35:15.982033Z",
     "shell.execute_reply": "2022-09-23T19:35:15.981108Z",
     "shell.execute_reply.started": "2022-09-23T19:35:15.973597Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rename year column on panes to avoid duplicate column name\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:16.826903Z",
     "iopub.status.busy": "2022-09-23T19:35:16.826446Z",
     "iopub.status.idle": "2022-09-23T19:35:17.211664Z",
     "shell.execute_reply": "2022-09-23T19:35:17.210952Z",
     "shell.execute_reply.started": "2022-09-23T19:35:16.826873Z"
    }
   },
   "outputs": [],
   "source": [
    "#join the flights and plane table use key as tailnum column\n",
    "model_data = flights.join(planes, on='tailnum', how='leftouter')\n",
    "model_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:17.668385Z",
     "iopub.status.busy": "2022-09-23T19:35:17.668013Z",
     "iopub.status.idle": "2022-09-23T19:35:20.933541Z",
     "shell.execute_reply": "2022-09-23T19:35:20.932684Z",
     "shell.execute_reply.started": "2022-09-23T19:35:17.668356Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:20.935237Z",
     "iopub.status.busy": "2022-09-23T19:35:20.934900Z",
     "iopub.status.idle": "2022-09-23T19:35:20.989598Z",
     "shell.execute_reply": "2022-09-23T19:35:20.988707Z",
     "shell.execute_reply.started": "2022-09-23T19:35:20.935203Z"
    }
   },
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn('arr_delay', model_data.arr_delay.cast('integer'))\n",
    "model_data = model_data.withColumn('air_time' , model_data.air_time.cast('integer'))\n",
    "model_data = model_data.withColumn('month', model_data.month.cast('integer'))\n",
    "model_data = model_data.withColumn('plane_year', model_data.plane_year.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:24.457990Z",
     "iopub.status.busy": "2022-09-23T19:35:24.457564Z",
     "iopub.status.idle": "2022-09-23T19:35:24.938741Z",
     "shell.execute_reply": "2022-09-23T19:35:24.937790Z",
     "shell.execute_reply.started": "2022-09-23T19:35:24.457957Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_data.describe('arr_delay', 'air_time','month', 'plane_year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:24.940496Z",
     "iopub.status.busy": "2022-09-23T19:35:24.940221Z",
     "iopub.status.idle": "2022-09-23T19:35:24.962359Z",
     "shell.execute_reply": "2022-09-23T19:35:24.961178Z",
     "shell.execute_reply.started": "2022-09-23T19:35:24.940472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column\n",
    "model_data =model_data.withColumn('plane_age', model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:25.421341Z",
     "iopub.status.busy": "2022-09-23T19:35:25.420957Z",
     "iopub.status.idle": "2022-09-23T19:35:25.469730Z",
     "shell.execute_reply": "2022-09-23T19:35:25.468788Z",
     "shell.execute_reply.started": "2022-09-23T19:35:25.421311Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_data = model_data.withColumn('is_late', model_data.arr_delay >0)\n",
    "\n",
    "model_data = model_data.withColumn('label', model_data.is_late.cast('integer'))\n",
    "\n",
    "model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:26.098664Z",
     "iopub.status.busy": "2022-09-23T19:35:26.098260Z",
     "iopub.status.idle": "2022-09-23T19:35:26.374288Z",
     "shell.execute_reply": "2022-09-23T19:35:26.372972Z",
     "shell.execute_reply.started": "2022-09-23T19:35:26.098630Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:26.868953Z",
     "iopub.status.busy": "2022-09-23T19:35:26.868347Z",
     "iopub.status.idle": "2022-09-23T19:35:26.908021Z",
     "shell.execute_reply": "2022-09-23T19:35:26.907011Z",
     "shell.execute_reply.started": "2022-09-23T19:35:26.868920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "#Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carr_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:27.416200Z",
     "iopub.status.busy": "2022-09-23T19:35:27.415767Z",
     "iopub.status.idle": "2022-09-23T19:35:27.429150Z",
     "shell.execute_reply": "2022-09-23T19:35:27.428035Z",
     "shell.execute_reply.started": "2022-09-23T19:35:27.416167Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# encode the dest column just like you did above\n",
    "dest_indexer = StringIndexer(inputCol='dest', outputCol='dest_index')\n",
    "dest_encoder = OneHotEncoder(inputCol='dest_index', outputCol='dest_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:35:28.025207Z",
     "iopub.status.busy": "2022-09-23T19:35:28.024775Z",
     "iopub.status.idle": "2022-09-23T19:35:28.030488Z",
     "shell.execute_reply": "2022-09-23T19:35:28.028938Z",
     "shell.execute_reply.started": "2022-09-23T19:35:28.025177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble a  Vector\n",
    "from pyspark.ml.feature import  VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:33.027835Z",
     "iopub.status.busy": "2022-09-23T19:42:33.027433Z",
     "iopub.status.idle": "2022-09-23T19:42:33.039754Z",
     "shell.execute_reply": "2022-09-23T19:42:33.038229Z",
     "shell.execute_reply.started": "2022-09-23T19:42:33.027781Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vec_assembler =VectorAssembler(inputCols=['month', 'air_time','carr_fact','dest_fact','plane_age'],\n",
    "                              outputCol='features',handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:34.103934Z",
     "iopub.status.busy": "2022-09-23T19:42:34.103531Z",
     "iopub.status.idle": "2022-09-23T19:42:34.109812Z",
     "shell.execute_reply": "2022-09-23T19:42:34.108736Z",
     "shell.execute_reply.started": "2022-09-23T19:42:34.103900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #### Create the pipeline\n",
    "# You're finally ready to create a` Pipeline!` Pipeline is a class in the `pyspark.ml module` that combines all the Estimators and Transformers that you've already created.\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:34.747250Z",
     "iopub.status.busy": "2022-09-23T19:42:34.746844Z",
     "iopub.status.idle": "2022-09-23T19:42:35.569772Z",
     "shell.execute_reply": "2022-09-23T19:42:35.569024Z",
     "shell.execute_reply.started": "2022-09-23T19:42:34.747217Z"
    }
   },
   "outputs": [],
   "source": [
    "piped_data =flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:35.571187Z",
     "iopub.status.busy": "2022-09-23T19:42:35.570913Z",
     "iopub.status.idle": "2022-09-23T19:42:35.852219Z",
     "shell.execute_reply": "2022-09-23T19:42:35.851188Z",
     "shell.execute_reply.started": "2022-09-23T19:42:35.571153Z"
    }
   },
   "outputs": [],
   "source": [
    "piped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:35.853748Z",
     "iopub.status.busy": "2022-09-23T19:42:35.853440Z",
     "iopub.status.idle": "2022-09-23T19:42:35.872459Z",
     "shell.execute_reply": "2022-09-23T19:42:35.871322Z",
     "shell.execute_reply.started": "2022-09-23T19:42:35.853719Z"
    }
   },
   "outputs": [],
   "source": [
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:36.492164Z",
     "iopub.status.busy": "2022-09-23T19:42:36.491785Z",
     "iopub.status.idle": "2022-09-23T19:42:36.503094Z",
     "shell.execute_reply": "2022-09-23T19:42:36.501950Z",
     "shell.execute_reply.started": "2022-09-23T19:42:36.492133Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:37.310891Z",
     "iopub.status.busy": "2022-09-23T19:42:37.310209Z",
     "iopub.status.idle": "2022-09-23T19:42:37.322351Z",
     "shell.execute_reply": "2022-09-23T19:42:37.320876Z",
     "shell.execute_reply.started": "2022-09-23T19:42:37.310845Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #### Create the evaluator\n",
    "# The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the `BinaryClassificationEvaluator` from the `pyspark.ml.evaluation` module. This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number.\n",
    "\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:37.862538Z",
     "iopub.status.busy": "2022-09-23T19:42:37.862173Z",
     "iopub.status.idle": "2022-09-23T19:42:37.869944Z",
     "shell.execute_reply": "2022-09-23T19:42:37.868630Z",
     "shell.execute_reply.started": "2022-09-23T19:42:37.862509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:39.106197Z",
     "iopub.status.busy": "2022-09-23T19:42:39.105787Z",
     "iopub.status.idle": "2022-09-23T19:42:39.111550Z",
     "shell.execute_reply": "2022-09-23T19:42:39.110506Z",
     "shell.execute_reply.started": "2022-09-23T19:42:39.106166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:42:58.543452Z",
     "iopub.status.busy": "2022-09-23T19:42:58.542365Z",
     "iopub.status.idle": "2022-09-23T19:44:13.173769Z",
     "shell.execute_reply": "2022-09-23T19:44:13.172871Z",
     "shell.execute_reply.started": "2022-09-23T19:42:58.543416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:44:20.591457Z",
     "iopub.status.busy": "2022-09-23T19:44:20.590744Z",
     "iopub.status.idle": "2022-09-23T19:44:20.598827Z",
     "shell.execute_reply": "2022-09-23T19:44:20.596504Z",
     "shell.execute_reply.started": "2022-09-23T19:44:20.591399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:44:22.155229Z",
     "iopub.status.busy": "2022-09-23T19:44:22.154679Z",
     "iopub.status.idle": "2022-09-23T19:44:22.856000Z",
     "shell.execute_reply": "2022-09-23T19:44:22.854582Z",
     "shell.execute_reply.started": "2022-09-23T19:44:22.155186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "# \n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
